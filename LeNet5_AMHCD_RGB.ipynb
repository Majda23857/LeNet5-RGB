{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "752cf989",
   "metadata": {},
   "source": [
    "\n",
    "# LeNet‑5 sur AMHCD (RGB) — Notebook\n",
    "\n",
    "**Objectif** : entraîner et évaluer une implémentation Keras de LeNet‑5 adaptée à **32×32×3** sur le dataset **AMHCD** (Tifinagh manuscrit).\n",
    "\n",
    "> **Remarque importante** : ce notebook n'a pas d'accès internet ici. Téléchargez d'abord le dataset depuis Kaggle\n",
    "> (https://www.kaggle.com/datasets/benaddym/amazigh-handwritten-character-database-amhcd) puis placez-le localement.\n",
    "> Deux façons de procéder :\n",
    "> 1. **Dossiers par classe** : `data/AMHCD/train/<classe>/*.png`, `data/AMHCD/val/<classe>/*.png`, `data/AMHCD/test/<classe>/*.png`\n",
    "> 2. **Un seul dossier** : `data/AMHCD/all/<classe>/*.png` (on fera le split stratifié dans le notebook).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056a5499",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, random, math, json, itertools, time, shutil, glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "\n",
    "# Reproductibilité\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print(tf.__version__)\n",
    "device_name = tf.test.gpu_device_name()\n",
    "print(\"GPU:\", device_name if device_name else \"CPU only\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8e7e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === CONFIG UTILISATEUR ===\n",
    "CFG = {\n",
    "    \"root_dir\": \"data/AMHCD\",   # dossier racine\n",
    "    \"train_subdir\": \"train\",    # si vous avez déjà un split\n",
    "    \"val_subdir\": \"val\",\n",
    "    \"test_subdir\": \"test\",\n",
    "    \"all_subdir\": \"all\",        # sinon, mettez tout ici par classe\n",
    "    \"img_size\": 32,\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 30,\n",
    "    \"augment\": True,\n",
    "    \"optimizer\": \"adam\",        # \"sgd\" ou \"adam\"\n",
    "    \"initial_lr\": 1e-3,         # 1e-2 si SGD\n",
    "    \"use_maxpool\": True,        # False -> AveragePooling\n",
    "    \"use_relu\": True,           # False -> tanh\n",
    "    \"dropout\": 0.5,\n",
    "    \"weight_decay\": 0.0,        # L2 si > 0\n",
    "    \"patience\": 7\n",
    "}\n",
    "CFG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74656e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Tuple, List, Dict\n",
    "import pathlib\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def has_pre_split(root):\n",
    "    return (pathlib.Path(root)/CFG[\"train_subdir\"]).exists() and (pathlib.Path(root)/CFG[\"val_subdir\"]).exists()\n",
    "\n",
    "def get_class_names(root):\n",
    "    # Peek into one split (train or all) to infer classes\n",
    "    base = pathlib.Path(root)/ (CFG[\"train_subdir\"] if has_pre_split(root) else CFG[\"all_subdir\"])\n",
    "    classes = sorted([p.name for p in base.iterdir() if p.is_dir()])\n",
    "    return classes\n",
    "\n",
    "def make_datasets(root:str):\n",
    "    root = pathlib.Path(root)\n",
    "    img_size = (CFG[\"img_size\"], CFG[\"img_size\"])\n",
    "    batch_size = CFG[\"batch_size\"]\n",
    "    if has_pre_split(root):\n",
    "        train_dir = root/CFG[\"train_subdir\"]\n",
    "        val_dir   = root/CFG[\"val_subdir\"]\n",
    "        test_dir  = root/CFG[\"test_subdir\"]\n",
    "        train_ds = keras.preprocessing.image_dataset_from_directory(\n",
    "            train_dir, image_size=img_size, batch_size=batch_size, seed=SEED, label_mode=\"categorical\")\n",
    "        val_ds = keras.preprocessing.image_dataset_from_directory(\n",
    "            val_dir, image_size=img_size, batch_size=batch_size, seed=SEED, label_mode=\"categorical\")\n",
    "        test_ds = keras.preprocessing.image_dataset_from_directory(\n",
    "            test_dir, image_size=img_size, batch_size=batch_size, seed=SEED, label_mode=\"categorical\", shuffle=False)\n",
    "        class_names = train_ds.class_names\n",
    "    else:\n",
    "        all_dir = root/CFG[\"all_subdir\"]\n",
    "        all_ds = keras.preprocessing.image_dataset_from_directory(\n",
    "            all_dir, image_size=img_size, batch_size=batch_size, seed=SEED, label_mode=\"categorical\")\n",
    "        class_names = all_ds.class_names\n",
    "        # split 70/15/15\n",
    "        card = tf.data.experimental.cardinality(all_ds).numpy()\n",
    "        n_train = int(0.7 * card)\n",
    "        n_val   = int(0.15 * card)\n",
    "        train_ds = all_ds.take(n_train)\n",
    "        rem = all_ds.skip(n_train)\n",
    "        val_ds = rem.take(n_val)\n",
    "        test_ds = rem.skip(n_val)\n",
    "    num_classes = len(class_names)\n",
    "    print(\"Classes:\", class_names)\n",
    "    print(\"N classes:\", num_classes)\n",
    "\n",
    "    def norm(x,y): return (tf.cast(x, tf.float32)/255.0, y)\n",
    "    train_ds = train_ds.map(norm, num_parallel_calls=AUTOTUNE)\n",
    "    val_ds   = val_ds.map(norm,   num_parallel_calls=AUTOTUNE)\n",
    "    test_ds  = test_ds.map(norm,  num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    if CFG[\"augment\"]:\n",
    "        aug = keras.Sequential([\n",
    "            layers.RandomRotation(0.05, fill_mode=\"nearest\"),\n",
    "            layers.RandomTranslation(0.05,0.05, fill_mode=\"nearest\"),\n",
    "            layers.RandomZoom(0.05),\n",
    "            layers.RandomBrightness(factor=0.1)\n",
    "        ], name=\"augmentation\")\n",
    "        train_ds = train_ds.map(lambda x,y: (aug(x, training=True), y), num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    train_ds = train_ds.shuffle(1000, seed=SEED).prefetch(AUTOTUNE)\n",
    "    val_ds   = val_ds.prefetch(AUTOTUNE)\n",
    "    test_ds  = test_ds.prefetch(AUTOTUNE)\n",
    "    return train_ds, val_ds, test_ds, class_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef48a9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_lenet5(input_shape=(32,32,3), num_classes=33):\n",
    "    Pool = layers.MaxPooling2D if CFG[\"use_maxpool\"] else layers.AveragePooling2D\n",
    "    Act  = layers.ReLU if CFG[\"use_relu\"] else layers.Activation\n",
    "    wd = CFG[\"weight_decay\"]\n",
    "    reg = regularizers.l2(wd) if wd and wd>0 else None\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(6, kernel_size=5, padding=\"valid\", kernel_regularizer=reg)(inputs)\n",
    "    x = (Act() if CFG[\"use_relu\"] else Act(\"tanh\"))(x)\n",
    "    x = Pool(pool_size=2, strides=2)(x)\n",
    "\n",
    "    x = layers.Conv2D(16, kernel_size=5, padding=\"valid\", kernel_regularizer=reg)(x)\n",
    "    x = (Act() if CFG[\"use_relu\"] else Act(\"tanh\"))(x)\n",
    "    x = Pool(pool_size=2, strides=2)(x)\n",
    "\n",
    "    x = layers.Conv2D(120, kernel_size=5, padding=\"valid\", kernel_regularizer=reg)(x)\n",
    "    x = (Act() if CFG[\"use_relu\"] else Act(\"tanh\"))(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(84, kernel_regularizer=reg)(x)\n",
    "    x = (Act() if CFG[\"use_relu\"] else Act(\"tanh\"))(x)\n",
    "    if CFG[\"dropout\"] and CFG[\"dropout\"]>0:\n",
    "        x = layers.Dropout(CFG[\"dropout\"])(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs, name=\"LeNet5_AMHCD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36694198",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root = CFG[\"root_dir\"]\n",
    "train_ds, val_ds, test_ds, class_names = make_datasets(root)\n",
    "num_classes = len(class_names)\n",
    "\n",
    "model = build_lenet5(input_shape=(CFG[\"img_size\"], CFG[\"img_size\"], 3), num_classes=num_classes)\n",
    "model.summary()\n",
    "\n",
    "if CFG[\"optimizer\"].lower() == \"sgd\":\n",
    "    opt = keras.optimizers.SGD(learning_rate=CFG[\"initial_lr\"], momentum=0.9, nesterov=True)\n",
    "else:\n",
    "    opt = keras.optimizers.Adam(learning_rate=CFG[\"initial_lr\"])\n",
    "\n",
    "model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\", keras.metrics.TopKCategoricalAccuracy(k=3, name=\"top3\")])\n",
    "\n",
    "ckpt_path = \"lenet5_amhcd_best.keras\"\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(ckpt_path, monitor=\"val_accuracy\", save_best_only=True, verbose=1),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=max(2, CFG[\"patience\"]//2), verbose=1),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=CFG[\"patience\"], restore_best_weights=True, verbose=1)\n",
    "]\n",
    "\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=CFG[\"epochs\"], callbacks=callbacks)\n",
    "model.save(\"lenet5_amhcd_final.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc53e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "h = history.history\n",
    "# Accuracy\n",
    "plt.figure()\n",
    "plt.plot(h[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(h[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.legend(); plt.title(\"Accuracy\")\n",
    "plt.savefig(\"curve_accuracy.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Loss\n",
    "plt.figure()\n",
    "plt.plot(h[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(h[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.title(\"Loss\")\n",
    "plt.savefig(\"curve_loss.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Top-3\n",
    "plt.figure()\n",
    "plt.plot(h[\"top3\"], label=\"train_top3\")\n",
    "plt.plot(h[\"val_top3\"], label=\"val_top3\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Top-3 Acc\"); plt.legend(); plt.title(\"Top-3 Accuracy\")\n",
    "plt.savefig(\"curve_top3.png\", dpi=150)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facdf7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Évaluation\n",
    "test_loss, test_acc, test_top3 = model.evaluate(test_ds, verbose=0)\n",
    "print(f\"Test accuracy: {test_acc:.4f} | Top-3: {test_top3:.4f} | Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Prédictions\n",
    "y_true = []\n",
    "for _, y in test_ds:\n",
    "    y_true.append(np.argmax(y.numpy(), axis=1))\n",
    "y_true = np.concatenate(y_true, axis=0)\n",
    "\n",
    "y_pred_prob = model.predict(test_ds, verbose=0)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "np.save(\"confusion_matrix.npy\", cm)\n",
    "\n",
    "# Affichage matrice de confusion (matplotlib, sans style/couleurs personnalisés)\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(cm, interpolation=\"nearest\")\n",
    "plt.title(\"Matrice de confusion\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names, rotation=90)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "plt.tight_layout()\n",
    "plt.xlabel(\"Prédit\")\n",
    "plt.ylabel(\"Vrai\")\n",
    "plt.savefig(\"confusion_matrix.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f33e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Grille d'exemples correctement et mal classés\n",
    "import math\n",
    "\n",
    "# Récupérer les images et labels originaux du test_ds\n",
    "imgs = []\n",
    "labels = []\n",
    "for x, y in test_ds.unbatch().take(200):  # limite pour performance\n",
    "    imgs.append(x.numpy())\n",
    "    labels.append(np.argmax(y.numpy()))\n",
    "imgs = np.stack(imgs, axis=0)\n",
    "labels = np.array(labels)\n",
    "\n",
    "preds = np.argmax(model.predict(imgs, verbose=0), axis=1)\n",
    "\n",
    "correct_idx = np.where(preds == labels)[0][:25]\n",
    "wrong_idx   = np.where(preds != labels)[0][:25]\n",
    "\n",
    "def plot_grid(indices, title, filename):\n",
    "    n = len(indices)\n",
    "    cols = 5\n",
    "    rows = int(math.ceil(n/cols))\n",
    "    plt.figure(figsize=(cols*2, rows*2))\n",
    "    for i, idx in enumerate(indices):\n",
    "        plt.subplot(rows, cols, i+1)\n",
    "        plt.imshow(imgs[idx])\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"GT:{class_names[labels[idx]]}\\nP:{class_names[preds[idx]]}\")\n",
    "    plt.suptitle(title)\n",
    "    plt.savefig(filename, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "plot_grid(correct_idx, \"Exemples bien classés\", \"qualitative_correct.png\")\n",
    "plot_grid(wrong_idx,   \"Exemples mal classés\", \"qualitative_wrong.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf34abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summary = {\n",
    "    \"test_accuracy\": float(test_acc),\n",
    "    \"test_top3\": float(test_top3),\n",
    "    \"classes\": class_names,\n",
    "    \"config\": CFG\n",
    "}\n",
    "with open(\"results_summary.json\",\"w\") as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Artifacts enregistrés:\",\n",
    "      [\"lenet5_amhcd_best.keras\", \"lenet5_amhcd_final.keras\",\n",
    "       \"curve_accuracy.png\",\"curve_loss.png\",\"curve_top3.png\",\n",
    "       \"confusion_matrix.png\",\"qualitative_correct.png\",\"qualitative_wrong.png\",\n",
    "       \"results_summary.json\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dacc39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (Optionnel) Grad-CAM simple sur la dernière couche conv (Conv2D_120)\n",
    "# Pour aller vite, applique sur une seule image.\n",
    "def grad_cam(model, img_array, last_conv_layer_name=None, class_index=None):\n",
    "    if last_conv_layer_name is None:\n",
    "        # heuristique: trouver la dernière Conv2D\n",
    "        last_conv_layer_name = None\n",
    "        for layer in reversed(model.layers):\n",
    "            if isinstance(layer, layers.Conv2D):\n",
    "                last_conv_layer_name = layer.name\n",
    "                break\n",
    "    conv_layer = model.get_layer(last_conv_layer_name)\n",
    "\n",
    "    grad_model = keras.models.Model([model.inputs], [conv_layer.output, model.output])\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_out, preds = grad_model(img_array, training=False)\n",
    "        if class_index is None:\n",
    "            class_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, class_index]\n",
    "\n",
    "    grads = tape.gradient(class_channel, conv_out)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0,1,2))\n",
    "    conv_out = conv_out[0]\n",
    "    heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_out), axis=-1)\n",
    "    heatmap = tf.maximum(heatmap, 0) / (tf.reduce_max(heatmap) + 1e-8)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "# Démo (si test_ds non vide)\n",
    "for x, y in test_ds.take(1):\n",
    "    img = x[0:1]\n",
    "    hm = grad_cam(model, img)\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure()\n",
    "    plt.imshow(x[0].numpy())\n",
    "    plt.imshow(hm, alpha=0.3)\n",
    "    plt.title(\"Grad-CAM (superposé)\")\n",
    "    plt.savefig(\"gradcam_example.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
